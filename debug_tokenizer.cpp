#include <iostream>
#include <fstream>
#include <vector>
#include <string>
#include <sstream>
#include <unordered_map>

class SimpleTokenizer {
public:
    SimpleTokenizer(const std::string& vocabPath) {
        loadVocab(vocabPath);
    }
    
    std::vector<int64_t> encode(const std::string& text, int maxLength) {
        std::vector<int64_t> tokens;
        tokens.push_back(getTokenId("[CLS]"));
        
        std::istringstream iss(text);
        std::string word;
        
        while (iss >> word && tokens.size() < maxLength - 1) {
            // Split trailing punctuation from word
            while (!word.empty() && std::ispunct(word.back()) && tokens.size() < maxLength - 1) {
                char punct = word.back();
                word.pop_back();
                
                if (!word.empty()) {
                    std::string spToken = "\u2581" + word;
                    auto it = vocab_.find(spToken);
                    if (it != vocab_.end()) {
                        tokens.push_back(it->second);
                    } else {
                        tokens.push_back(getTokenId("[UNK]"));
                    }
                    word.clear();
                }
                
                std::string punctStr(1, punct);
                auto punct_it = vocab_.find(punctStr);
                if (punct_it != vocab_.end()) {
                    tokens.push_back(punct_it->second);
                } else {
                    tokens.push_back(getTokenId("[UNK]"));
                }
            }
            
            if (!word.empty()) {
                std::string spToken = "\u2581" + word;
                auto it = vocab_.find(spToken);
                if (it != vocab_.end()) {
                    tokens.push_back(it->second);
                } else {
                    tokens.push_back(getTokenId("[UNK]"));
                }
            }
        }
        
        tokens.push_back(getTokenId("[SEP]"));
        return tokens;
    }

private:
    std::unordered_map<std::string, int64_t> vocab_;
    
    void loadVocab(const std::string& path) {
        std::ifstream file(path);
        std::string token;
        int64_t id = 0;
        
        while (std::getline(file, token)) {
            if (!token.empty()) {
                vocab_[token] = id++;
            }
        }
    }
    
    int64_t getTokenId(const std::string& token) {
        auto it = vocab_.find(token);
        if (it != vocab_.end()) {
            return it->second;
        }
        auto unk_it = vocab_.find("[UNK]");
        return unk_it != vocab_.end() ? unk_it->second : 3;
    }
};

int main() {
    std::string vocabPath = std::string(getenv("HOME")) + "/.local/share/ModAI/ModAI/data/models/vocab.txt";
    SimpleTokenizer tokenizer(vocabPath);
    
    std::string text = "AI detection refers to the process of identifying whether a given piece of content has been generated by artificial intelligence.";
    
    auto tokens = tokenizer.encode(text, 768);
    
    std::cout << "C++ tokenizer output (first 25 tokens):" << std::endl;
    for (size_t i = 0; i < std::min(size_t(25), tokens.size()); i++) {
        std::cout << "  " << i << ": ID=" << tokens[i] << std::endl;
    }
    
    return 0;
}
