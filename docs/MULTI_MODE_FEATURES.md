# ModAI Multi-Mode Features

## Overview

ModAI now includes **4 powerful modes** for content analysis and moderation:

1. **ğŸ“± Reddit Scraper & Moderator** - Original functionality
2. **ğŸ’¬ AI Chatbot with Railguard** - LLM with safety filtering
3. **ğŸ“ AI Text Detector** - Detect AI-generated text
4. **ğŸ–¼ï¸ AI Image Detector** - Detect AI-generated images

---

## Mode 1: Reddit Scraper & Moderator

**Purpose**: Scrape Reddit posts/comments and automatically moderate them using AI detection and Hive API.

### Features:
- Scrapes subreddits for posts and comments
- Detects AI-generated content
- Moderates for NSFW, hate speech, toxicity via Hive API
- Visual dashboard with filtering and search
- Manual override and review workflow

### Usage:
1. Enter a subreddit name (e.g., `technology`)
2. Click "Start Scraping"
3. View results in real-time table
4. Click items to see detailed analysis

---

## Mode 2: AI Chatbot with Railguard ğŸ’¬

**Purpose**: Chat with an LLM (Large Language Model) where all responses are filtered through Hive API to block offensive content.

### Features:
- **LLM Integration**: Supports Ollama (local) or OpenAI-compatible APIs
- **Railguard Safety**: Every AI response is analyzed by Hive API
- **Automatic Blocking**: Offensive responses are blocked before reaching the user
- **Conversation Context**: Maintains chat history for coherent conversations

### How It Works:
1. User sends a message â†’ LLM generates response
2. Response is analyzed by **Hive Text Moderation API**
3. If score > threshold (default 0.7): **ğŸ›‘ Response Blocked**
4. If safe: Response is shown to user

### Setup:

#### Option A: Local LLM (Ollama)
```bash
# Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Pull a model (e.g., Llama 2)
ollama pull llama2

# Start Ollama (runs on localhost:11434)
ollama serve
```

The chatbot is pre-configured to use `http://localhost:11434/api/chat`.

#### Option B: OpenAI API
Modify the initialization in [MainWindow.cpp](src/ui/MainWindow.cpp):
```cpp
chatbotPanel_->initialize(
    std::make_unique<QtHttpClient>(this),
    std::make_unique<HiveTextModerator>(...),
    "YOUR_OPENAI_API_KEY",
    "https://api.openai.com/v1/chat/completions"
);
```

And update the model in [ChatbotPanel.cpp](src/ui/ChatbotPanel.cpp):
```cpp
payload["model"] = "gpt-3.5-turbo"; // or gpt-4
```

### Configuration:
- **Moderation Threshold**: Edit `threshold` in [ChatbotPanel.cpp](src/ui/ChatbotPanel.cpp#L160)
- **System Prompt**: Customize assistant behavior in [ChatbotPanel.cpp](src/ui/ChatbotPanel.cpp#L118)

---

## Mode 3: AI Text Detector ğŸ“

**Purpose**: Paste any text to get an AI-detection score showing likelihood it was written by AI.

### Features:
- **Local ONNX Model**: Uses `desklib/ai-text-detector-v1.01` transformer model
- **Fast Inference**: Runs locally without API calls
- **Visual Score Bar**: 0-100% confidence indicator
- **Interpretation Guide**: Clear verdicts (Human vs AI)

### How It Works:
1. Paste text (minimum 10 words recommended)
2. Click "Analyze Text"
3. View AI score:
   - **0-30%**: Likely human-written âœ…
   - **30-50%**: Mixed/uncertain ğŸ¤”
   - **50-80%**: Possibly AI-generated âš ï¸
   - **80-100%**: Likely AI-generated ğŸ¤–

### Model Setup:
The AI text detector requires an ONNX model. Export it using:
```bash
python3 scripts/export_model_to_onnx.py --output ~/.local/share/ModAI/data/models
```

This downloads the Hugging Face model `desklib/ai-text-detector-v1.01` and converts it to ONNX format.

---

## Mode 4: AI Image Detector ğŸ–¼ï¸

**Purpose**: Upload images to detect if they were generated by AI (e.g., Stable Diffusion, DALL-E, Midjourney).

### Features:
- **Image Upload**: Supports PNG, JPG, JPEG, GIF, BMP, WebP
- **Visual Preview**: See the image before analysis
- **AI Detection Score**: Confidence that image is AI-generated
- **Hive AI API**: Uses Hive's computer vision models

### How It Works:
1. Click "Select Image" and choose a file
2. Click "Analyze Image"
3. Hive API analyzes visual patterns, artifacts, inconsistencies
4. View AI score:
   - **0-30%**: Likely authentic/real âœ…
   - **30-50%**: Mixed/uncertain ğŸ¤”
   - **50-80%**: Possibly AI-generated âš ï¸
   - **80-100%**: Likely AI-generated ğŸ¤–

### Note:
Requires Hive API to include `ai_generated` class. If not available, the detector uses heuristics based on other image properties.

---

## Architecture

### Tab-Based UI
The application uses `QTabWidget` with 4 tabs for easy mode switching:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Reddit Scraper] [Chatbot] [Text] [Image]â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                         â”‚
â”‚         MODE-SPECIFIC CONTENT           â”‚
â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Shared Components
All modes share these services:
- **HttpClient**: For API calls (Hive, LLM)
- **TextDetector**: Local ONNX AI text detection
- **ImageModerator**: Hive image moderation API
- **TextModerator**: Hive text moderation API
- **Logger**: Centralized logging
- **Crypto**: Secure API key storage

### Panel Architecture
Each mode is a self-contained `QWidget`:
- `ChatbotPanel`: Chat UI + LLM integration
- `AITextDetectorPanel`: Text input + analysis
- `AIImageDetectorPanel`: Image upload + analysis
- Reddit scraper: Embedded in MainWindow

---

## API Keys Required

### Hive API (Required for all modes)
```bash
# Set your Hive API key
# The app uses Qt's secure storage
```

On first run, you'll be prompted to enter your Hive API key. Get one at:
https://thehive.ai/

### LLM API (Optional - only for chatbot)
- **Local Ollama**: No API key needed
- **OpenAI**: Requires OpenAI API key

---

## Building with New Modes

The CMakeLists.txt has been updated to include all new files:

```bash
cd build
cmake ..
make -j$(nproc)
./ModAI
```

### New Files:
```
include/ui/
  - ChatbotPanel.h
  - AITextDetectorPanel.h  
  - AIImageDetectorPanel.h
  - RedditScraperPanel.h (placeholder for future refactor)

src/ui/
  - ChatbotPanel.cpp
  - AITextDetectorPanel.cpp
  - AIImageDetectorPanel.cpp
```

---

## Future Enhancements

### Potential New Modes:
1. **Video Moderation**: Upload videos for frame-by-frame analysis
2. **Batch Processing**: Bulk analyze folders of content
3. **Real-time Stream Monitoring**: Monitor live social media feeds
4. **Custom Model Training**: Fine-tune detection models on your data
5. **Export & Reporting**: Generate PDF/Excel reports
6. **API Server Mode**: Run ModAI as a REST API service

### Chatbot Enhancements:
- Multiple LLM provider support (Anthropic, Cohere, local models)
- Conversation export/import
- Custom system prompts per conversation
- Token usage tracking
- Streaming responses

### Detector Enhancements:
- Multi-model ensemble for better accuracy
- Watermark detection for images
- Metadata analysis
- Batch image processing
- Drag-and-drop file upload

---

## Troubleshooting

### Chatbot Issues:

**Problem**: "Error: HTTP client not initialized"
- **Solution**: Ensure Hive API key is set

**Problem**: "LLM Error" 
- **Solution**: Check Ollama is running: `ollama serve`
- Verify model is installed: `ollama list`

**Problem**: Responses always blocked
- **Solution**: Lower threshold in ChatbotPanel.cpp or check Hive API status

### Text Detector Issues:

**Problem**: "AI detector not available"
- **Solution**: Export ONNX model:
  ```bash
  python3 scripts/export_model_to_onnx.py --output ~/.local/share/ModAI/data/models
  ```

**Problem**: "Text too short" warning
- **Solution**: Provide at least 10 words for analysis

### Image Detector Issues:

**Problem**: "AI detector not available"
- **Solution**: Set Hive API key (image detection uses Hive)

**Problem**: Low accuracy
- **Solution**: Hive API's `ai_generated` class may not be available. Contact Hive support.

---

## Performance Notes

- **Chatbot**: ~1-5 seconds per response (depends on LLM)
- **Text Detection**: ~100-500ms locally (ONNX)
- **Image Detection**: ~1-2 seconds (Hive API)
- **Reddit Scraping**: Respects rate limits (60 requests/min)

---

## Privacy & Security

- **Local AI Detection**: Text analysis runs locally - no data leaves your machine
- **API Keys**: Stored securely using Qt's `QSettings` + encryption
- **No Telemetry**: ModAI does not send usage data anywhere
- **Conversation History**: Chatbot history stored in memory only (not persisted)

---

## License

Same as parent project (MIT/GPL/etc.)

---

## Credits

- **ONNX Runtime**: Microsoft
- **Hive AI API**: TheHive.ai
- **Ollama**: Ollama.ai
- **Hugging Face Model**: desklib/ai-text-detector-v1.01
- **Qt Framework**: The Qt Company

---

## Support

For issues, questions, or feature requests:
1. Check the logs: `~/.local/share/ModAI/data/logs/system.log`
2. Open a GitHub issue
3. Contact: [Your contact info]

---

**Enjoy your multi-mode AI content analysis tool! ğŸš€**
